package com.tibtech.nifi.processors.standard;

import groovy.lang.Closure;
import groovy.lang.DelegatesTo;
import java.lang.String;
import java.util.HashMap;
import java.util.Map;
import java.util.function.Function;

public final class PutSQL {
  /**
   * The component type name.
   */
  public static final String COMPONENT_TYPE = "org.apache.nifi.processors.standard.PutSQL";

  /**
   * Specifies the JDBC Connection Pool to use in order to convert the JSON message to a SQL statement. The Connection Pool is necessary in order to determine the appropriate database column types.
   */
  public static final String JDBC_CONNECTION_POOL_PROPERTY = "JDBC Connection Pool";

  /**
   * The SQL statement to execute. The statement can be empty, a constant value, or built from attributes using Expression Language. If this property is specified, it will be used regardless of the content of incoming flowfiles. If this property is empty, the content of the incoming flow file is expected to contain a valid SQL statement, to be issued by the processor to the database.
   */
  public static final String PUTSQL_SQL_STATEMENT_PROPERTY = "putsql-sql-statement";

  /**
   * If true, when a FlowFile is consumed by this Processor, the Processor will first check the fragment.identifier and fragment.count attributes of that FlowFile. If the fragment.count value is greater than 1, the Processor will not process any FlowFile with that fragment.identifier until all are available; at that point, it will process all FlowFiles with that fragment.identifier as a single transaction, in the order specified by the FlowFiles' fragment.index attributes. This Provides atomicity of those SQL statements. If this value is false, these attributes will be ignored and the updates will occur independent of one another.
   */
  public static final String SUPPORT_FRAGMENTED_TRANSACTIONS_PROPERTY = "Support Fragmented Transactions";

  /**
   * If the <Support Fragmented Transactions> property is set to true, specifies how long to wait for all FlowFiles for a particular fragment.identifier attribute to arrive before just transferring all of the FlowFiles with that identifier to the 'failure' relationship
   */
  public static final String TRANSACTION_TIMEOUT_PROPERTY = "Transaction Timeout";

  /**
   * The preferred number of FlowFiles to put to the database in a single transaction
   */
  public static final String BATCH_SIZE_PROPERTY = "Batch Size";

  /**
   * If true, any key that is automatically generated by the database will be added to the FlowFile that generated it using the sql.generate.key attribute. This may result in slightly slower performance and is not supported by all databases.
   */
  public static final String OBTAIN_GENERATED_KEYS_PROPERTY = "Obtain Generated Keys";

  /**
   * Specify how to handle error. By default (false), if an error occurs while processing a FlowFile, the FlowFile will be routed to 'failure' or 'retry' relationship based on error type, and processor can continue with next FlowFile. Instead, you may want to rollback currently processed FlowFiles and stop further processing immediately. In that case, you can do so by enabling this 'Rollback On Failure' property.  If enabled, failed FlowFiles will stay in the input relationship without penalizing it and being processed repeatedly until it gets processed successfully or removed by other means. It is important to set adequate 'Yield Duration' to avoid retrying too frequently.
   */
  public static final String ROLLBACK_ON_FAILURE_PROPERTY = "rollback-on-failure";

  private final Map<String, String> properties;

  public PutSQL() {
    this.properties = new HashMap<>();
  }

  public PutSQL(final Map<String, String> properties) {
    this.properties = new HashMap<>(properties);
  }

  /**
   * Specifies the JDBC Connection Pool to use in order to convert the JSON message to a SQL statement. The Connection Pool is necessary in order to determine the appropriate database column types.
   */
  public final String getJdbcConnectionPool() {
    return properties.get(JDBC_CONNECTION_POOL_PROPERTY);
  }

  /**
   * Specifies the JDBC Connection Pool to use in order to convert the JSON message to a SQL statement. The Connection Pool is necessary in order to determine the appropriate database column types.
   */
  public final PutSQL setJdbcConnectionPool(final String jdbcConnectionPool) {
    properties.put(JDBC_CONNECTION_POOL_PROPERTY, jdbcConnectionPool);
    return this;
  }

  /**
   * Specifies the JDBC Connection Pool to use in order to convert the JSON message to a SQL statement. The Connection Pool is necessary in order to determine the appropriate database column types.
   */
  public final PutSQL removeJdbcConnectionPool() {
    properties.remove(JDBC_CONNECTION_POOL_PROPERTY);
    return this;
  }

  /**
   * The SQL statement to execute. The statement can be empty, a constant value, or built from attributes using Expression Language. If this property is specified, it will be used regardless of the content of incoming flowfiles. If this property is empty, the content of the incoming flow file is expected to contain a valid SQL statement, to be issued by the processor to the database.
   */
  public final String getPutsqlSqlStatement() {
    return properties.get(PUTSQL_SQL_STATEMENT_PROPERTY);
  }

  /**
   * The SQL statement to execute. The statement can be empty, a constant value, or built from attributes using Expression Language. If this property is specified, it will be used regardless of the content of incoming flowfiles. If this property is empty, the content of the incoming flow file is expected to contain a valid SQL statement, to be issued by the processor to the database.
   */
  public final PutSQL setPutsqlSqlStatement(final String putsqlSqlStatement) {
    properties.put(PUTSQL_SQL_STATEMENT_PROPERTY, putsqlSqlStatement);
    return this;
  }

  /**
   * The SQL statement to execute. The statement can be empty, a constant value, or built from attributes using Expression Language. If this property is specified, it will be used regardless of the content of incoming flowfiles. If this property is empty, the content of the incoming flow file is expected to contain a valid SQL statement, to be issued by the processor to the database.
   */
  public final PutSQL removePutsqlSqlStatement() {
    properties.remove(PUTSQL_SQL_STATEMENT_PROPERTY);
    return this;
  }

  /**
   * If true, when a FlowFile is consumed by this Processor, the Processor will first check the fragment.identifier and fragment.count attributes of that FlowFile. If the fragment.count value is greater than 1, the Processor will not process any FlowFile with that fragment.identifier until all are available; at that point, it will process all FlowFiles with that fragment.identifier as a single transaction, in the order specified by the FlowFiles' fragment.index attributes. This Provides atomicity of those SQL statements. If this value is false, these attributes will be ignored and the updates will occur independent of one another.
   */
  public final String getSupportFragmentedTransactions() {
    return properties.get(SUPPORT_FRAGMENTED_TRANSACTIONS_PROPERTY);
  }

  /**
   * If true, when a FlowFile is consumed by this Processor, the Processor will first check the fragment.identifier and fragment.count attributes of that FlowFile. If the fragment.count value is greater than 1, the Processor will not process any FlowFile with that fragment.identifier until all are available; at that point, it will process all FlowFiles with that fragment.identifier as a single transaction, in the order specified by the FlowFiles' fragment.index attributes. This Provides atomicity of those SQL statements. If this value is false, these attributes will be ignored and the updates will occur independent of one another.
   */
  public final PutSQL setSupportFragmentedTransactions(final String supportFragmentedTransactions) {
    properties.put(SUPPORT_FRAGMENTED_TRANSACTIONS_PROPERTY, supportFragmentedTransactions);
    return this;
  }

  /**
   * If true, when a FlowFile is consumed by this Processor, the Processor will first check the fragment.identifier and fragment.count attributes of that FlowFile. If the fragment.count value is greater than 1, the Processor will not process any FlowFile with that fragment.identifier until all are available; at that point, it will process all FlowFiles with that fragment.identifier as a single transaction, in the order specified by the FlowFiles' fragment.index attributes. This Provides atomicity of those SQL statements. If this value is false, these attributes will be ignored and the updates will occur independent of one another.
   */
  public final PutSQL removeSupportFragmentedTransactions() {
    properties.remove(SUPPORT_FRAGMENTED_TRANSACTIONS_PROPERTY);
    return this;
  }

  /**
   * If the <Support Fragmented Transactions> property is set to true, specifies how long to wait for all FlowFiles for a particular fragment.identifier attribute to arrive before just transferring all of the FlowFiles with that identifier to the 'failure' relationship
   */
  public final String getTransactionTimeout() {
    return properties.get(TRANSACTION_TIMEOUT_PROPERTY);
  }

  /**
   * If the <Support Fragmented Transactions> property is set to true, specifies how long to wait for all FlowFiles for a particular fragment.identifier attribute to arrive before just transferring all of the FlowFiles with that identifier to the 'failure' relationship
   */
  public final PutSQL setTransactionTimeout(final String transactionTimeout) {
    properties.put(TRANSACTION_TIMEOUT_PROPERTY, transactionTimeout);
    return this;
  }

  /**
   * If the <Support Fragmented Transactions> property is set to true, specifies how long to wait for all FlowFiles for a particular fragment.identifier attribute to arrive before just transferring all of the FlowFiles with that identifier to the 'failure' relationship
   */
  public final PutSQL removeTransactionTimeout() {
    properties.remove(TRANSACTION_TIMEOUT_PROPERTY);
    return this;
  }

  /**
   * The preferred number of FlowFiles to put to the database in a single transaction
   */
  public final String getBatchSize() {
    return properties.get(BATCH_SIZE_PROPERTY);
  }

  /**
   * The preferred number of FlowFiles to put to the database in a single transaction
   */
  public final PutSQL setBatchSize(final String batchSize) {
    properties.put(BATCH_SIZE_PROPERTY, batchSize);
    return this;
  }

  /**
   * The preferred number of FlowFiles to put to the database in a single transaction
   */
  public final PutSQL removeBatchSize() {
    properties.remove(BATCH_SIZE_PROPERTY);
    return this;
  }

  /**
   * If true, any key that is automatically generated by the database will be added to the FlowFile that generated it using the sql.generate.key attribute. This may result in slightly slower performance and is not supported by all databases.
   */
  public final String getObtainGeneratedKeys() {
    return properties.get(OBTAIN_GENERATED_KEYS_PROPERTY);
  }

  /**
   * If true, any key that is automatically generated by the database will be added to the FlowFile that generated it using the sql.generate.key attribute. This may result in slightly slower performance and is not supported by all databases.
   */
  public final PutSQL setObtainGeneratedKeys(final String obtainGeneratedKeys) {
    properties.put(OBTAIN_GENERATED_KEYS_PROPERTY, obtainGeneratedKeys);
    return this;
  }

  /**
   * If true, any key that is automatically generated by the database will be added to the FlowFile that generated it using the sql.generate.key attribute. This may result in slightly slower performance and is not supported by all databases.
   */
  public final PutSQL removeObtainGeneratedKeys() {
    properties.remove(OBTAIN_GENERATED_KEYS_PROPERTY);
    return this;
  }

  /**
   * Specify how to handle error. By default (false), if an error occurs while processing a FlowFile, the FlowFile will be routed to 'failure' or 'retry' relationship based on error type, and processor can continue with next FlowFile. Instead, you may want to rollback currently processed FlowFiles and stop further processing immediately. In that case, you can do so by enabling this 'Rollback On Failure' property.  If enabled, failed FlowFiles will stay in the input relationship without penalizing it and being processed repeatedly until it gets processed successfully or removed by other means. It is important to set adequate 'Yield Duration' to avoid retrying too frequently.
   */
  public final String getRollbackOnFailure() {
    return properties.get(ROLLBACK_ON_FAILURE_PROPERTY);
  }

  /**
   * Specify how to handle error. By default (false), if an error occurs while processing a FlowFile, the FlowFile will be routed to 'failure' or 'retry' relationship based on error type, and processor can continue with next FlowFile. Instead, you may want to rollback currently processed FlowFiles and stop further processing immediately. In that case, you can do so by enabling this 'Rollback On Failure' property.  If enabled, failed FlowFiles will stay in the input relationship without penalizing it and being processed repeatedly until it gets processed successfully or removed by other means. It is important to set adequate 'Yield Duration' to avoid retrying too frequently.
   */
  public final PutSQL setRollbackOnFailure(final String rollbackOnFailure) {
    properties.put(ROLLBACK_ON_FAILURE_PROPERTY, rollbackOnFailure);
    return this;
  }

  /**
   * Specify how to handle error. By default (false), if an error occurs while processing a FlowFile, the FlowFile will be routed to 'failure' or 'retry' relationship based on error type, and processor can continue with next FlowFile. Instead, you may want to rollback currently processed FlowFiles and stop further processing immediately. In that case, you can do so by enabling this 'Rollback On Failure' property.  If enabled, failed FlowFiles will stay in the input relationship without penalizing it and being processed repeatedly until it gets processed successfully or removed by other means. It is important to set adequate 'Yield Duration' to avoid retrying too frequently.
   */
  public final PutSQL removeRollbackOnFailure() {
    properties.remove(ROLLBACK_ON_FAILURE_PROPERTY);
    return this;
  }

  public final String getDynamicProperty(final String name) {
    return properties.get(name);
  }

  public final PutSQL setDynamicProperty(final String name, final String value) {
    properties.put(name, value);
    return this;
  }

  public final PutSQL removeDynamicProperty(final String name) {
    properties.remove(name);
    return this;
  }

  public final Map<String, String> build() {
    return properties;
  }

  public static final Map<String, String> build(final Function<PutSQL, PutSQL> configurator) {
    return configurator.apply(new PutSQL()).build();
  }

  public static final Map<String, String> build(
      @DelegatesTo(strategy = Closure.DELEGATE_ONLY, value = PutSQL.class) final Closure<PutSQL> closure) {
    return build(c -> {
      final Closure<com.tibtech.nifi.processors.standard.PutSQL> code = closure.rehydrate(c, com.tibtech.nifi.processors.standard.PutSQL.class, com.tibtech.nifi.processors.standard.PutSQL.class);
      code.setResolveStrategy(Closure.DELEGATE_ONLY);
      code.call();
      return c;
    } );
  }

  public static final Map<String, String> update(final Map<String, String> properties,
      final Function<PutSQL, PutSQL> configurator) {
    return configurator.apply(new PutSQL(properties)).build();
  }

  public static final Map<String, String> update(final Map<String, String> properties,
      @DelegatesTo(strategy = Closure.DELEGATE_ONLY, value = PutSQL.class) final Closure<PutSQL> closure) {
    return update(properties, c -> {
      final Closure<com.tibtech.nifi.processors.standard.PutSQL> code = closure.rehydrate(c, com.tibtech.nifi.processors.standard.PutSQL.class, com.tibtech.nifi.processors.standard.PutSQL.class);
      code.setResolveStrategy(Closure.DELEGATE_ONLY);
      code.call();
      return c;
    } );
  }
}
